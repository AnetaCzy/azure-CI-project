{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8cc23f6-3073-48ab-93ab-9c856589a0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Problem Statement 01 - Data Loading and Basic Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac12c65f-69d9-4401-b9ee-a39a555e85df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load a CSV file containing retail transactions into a Spark DataFrame.\n",
    "azure_storage_account = \"stgaccaneta\"\n",
    "azure_container_products = \"products\" #json\n",
    "azure_container_transactions = \"transactions\" #csv\n",
    "azure_access_keys = \"xxx\"\n",
    "\n",
    " \n",
    "products_file_path = f\"abfss://{azure_container_products}@{azure_storage_account}.dfs.core.windows.net/products.json\"\n",
    "df_products = spark.read.option(\n",
    "    f\"fs.azure.account.key.{azure_storage_account}.dfs.core.windows.net\",\n",
    "    azure_access_keys,\n",
    ").json(path=products_file_path,\n",
    "     )\n",
    "\n",
    "transactions_file_path = f\"abfss://{azure_container_transactions}@{azure_storage_account}.dfs.core.windows.net/transactions.csv\"\n",
    "df_transactions = spark.read.option(\n",
    "    f\"fs.azure.account.key.{azure_storage_account}.dfs.core.windows.net\",\n",
    "    azure_access_keys,\n",
    ").csv(path=transactions_file_path,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d4fa3c-d746-411e-9dee-e47d300205dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e73d968-8c27-493f-93a2-068b86345134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_products.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87fb9e99-4ad6-495d-a914-f56ccb548f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2e05b9-d081-45e5-8d63-e039c15baa6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Azure project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15eed1cf-80d7-4c10-bbb4-41c684e17892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. After loading CSV, convert it to JSON.\n",
    "#loading transactions_df from volume because it gives: UnknownException: (shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.KeyProviderException) Failure to initialize configuration for storage account stgaccaneta.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.key ehwn changing to json\n",
    "df_transactions_2 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/Volumes/project_data/default/project_data/transactions.csv\")\n",
    "df_transactions_2.write.mode(\"overwrite\").json(\"/Volumes/project_data/default/project_data/transactions_json\")\n",
    "df_transactions_2.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d753b4f6-2a02-498a-b884-cc9348a384bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Show first 10 rows of transactions.\n",
    "df_transactions_2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b40f62a-fb62-4cb3-8b9c-0dc0a66da955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the total number of transactions.\n",
    "total_transactions = df_transactions_2.count()\n",
    "print(f\"Total number of transactions: {total_transactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c34867-3f30-48d3-90cc-ff7aaaa81f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Problem Statement 02 - Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecf9603-fd99-4eec-987b-cff5a082fa5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter transactions where the total amount is greater than $100\n",
    "from pyspark.sql.functions import col\n",
    "df_filtered_transactions = df_transactions_2.filter(col(\"total_amount\") > 100)\n",
    "df_filtered_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05657a1-96d0-467e-814a-50fc7a38f5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Select product names and prices for products that belong to the \"Electronics\" category\n",
    "df_electronics = df_products.filter(col(\"category\") == \"Electronics\").select(\"name\", \"price\")\n",
    "df_electronics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a4ec662-195a-4b5f-9fcb-bd35f3284f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## intermediate\n",
    "Problem Statement 01 - Data Loading and Basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb090934-e796-4066-a741-d3e50a5f7b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Join transactions and products on product_id\n",
    "df_joined = df_transactions_2.join(df_products, df_transactions_2.product_id == df_products.product_id, \"inner\")\n",
    "df_joined.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5cb1932-f771-49cb-870c-4cc20f779479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total sales amount for each product category\n",
    "from pyspark.sql.functions import sum\n",
    "df_category_sales = df_joined.groupBy(\"category\").agg(sum(\"total_amount\").alias(\"total_sales\"))\n",
    "df_category_sales.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e81997-cf44-4612-b15c-45a218ff356d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the average transaction amount for each day\n",
    "from pyspark.sql.functions import avg, to_date\n",
    "\n",
    "df_transactions_date = df_transactions_2.withColumn(\"date\", to_date(\"transaction_date\"))\n",
    "df_avg_per_day = df_transactions_date.groupBy(\"date\").agg(avg(\"total_amount\").alias(\"avg_transaction_amount\"))\n",
    "df_avg_per_day.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4251e65-730b-43ff-b64d-247e0741409f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4. Sort based on the average transaction amount\n",
    "df_avg_category = df_joined.groupBy(\"category\").agg(avg(\"total_amount\").alias(\"avg_transaction_amount\"))\n",
    "df_sorted_avg_category = df_avg_category.orderBy(\"avg_transaction_amount\", ascending=False)\n",
    "df_sorted_avg_category.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df67fe15-dd11-45d8-9992-855a87a4321e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Sort based on the average transaction amount\n",
    "df_sorted_avg = df_avg_per_day.orderBy(\"avg_transaction_amount\", ascending=False)\n",
    "df_sorted_avg.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ff14004-5ef9-496a-9059-c010ad5da6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project_azure_code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
